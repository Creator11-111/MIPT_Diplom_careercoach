# СПИСОК ИСПОЛЬЗОВАННЫХ ИСТОЧНИКОВ

## Рынок труда и карьерное консультирование

1. Автономов В.С. Рынок труда в условиях цифровой экономики // Вопросы экономики. — 2022. — № 3. — С. 45-62.

2. Cappelli P. Why Good People Can't Get Jobs: The Skills Gap and What Companies Can Do About It. — Wharton Digital Press, 2012. — 128 p.

3. Autor D.H., Dorn D. The Growth of Low-Skill Service Jobs and the Polarization of the US Labor Market // American Economic Review. — 2013. — Vol. 103, № 5. — P. 1553-1597.

4. Brynjolfsson E., McAfee A. The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies. — W. W. Norton & Company, 2014. — 336 p.

5. Frey C.B., Osborne M.A. The Future of Employment: How Susceptible Are Jobs to Computerisation? // Technological Forecasting and Social Change. — 2017. — Vol. 114. — P. 254-280.

6. LinkedIn Economic Graph Research. Future of Skills 2025. — LinkedIn, 2024.

7. World Economic Forum. The Future of Jobs Report 2023. — WEF, 2023.

8. HeadHunter Research. Рынок труда России 2024: тренды и прогнозы. — HH.ru, 2024.

9. Kessler R., Torres J., Karoui J. AI-Powered Career Recommendation Systems: A Survey // ACM Computing Surveys. — 2023. — Vol. 55, № 8. — P. 1-35.

10. Kenthapadi K., Le B., Venkataraman G. Personalized Job Recommendation System at LinkedIn // Proceedings of the 23rd ACM SIGKDD. — 2017. — P. 1655-1664.

## Языковые модели и NLP

11. Devlin J., Chang M.W., Lee K., Toutanova K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding // NAACL-HLT. — 2019. — P. 4171-4186.

12. Reimers N., Gurevych I. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks // EMNLP-IJCNLP. — 2019. — P. 3982-3992.

13. Brown T., Mann B., Ryder N. et al. Language Models are Few-Shot Learners // NeurIPS. — 2020. — Vol. 33. — P. 1877-1901.

14. OpenAI. GPT-4 Technical Report. — arXiv:2303.08774, 2023.

15. Яндекс. YandexGPT: Документация API. — https://cloud.yandex.ru/docs/yandexgpt/, 2024.

25. Vaswani A., Shazeer N., Parmar N. et al. Attention Is All You Need // NeurIPS. — 2017. — Vol. 30. — P. 5998-6008.

26. Mikolov T., Chen K., Corrado G., Dean J. Efficient Estimation of Word Representations in Vector Space // ICLR Workshop. — 2013.

27. Pennington J., Socher R., Manning C.D. GloVe: Global Vectors for Word Representation // EMNLP. — 2014. — P. 1532-1543.

28. Peters M.E., Neumann M., Iyyer M. et al. Deep Contextualized Word Representations // NAACL-HLT. — 2018. — P. 2227-2237.

29. Radford A., Narasimhan K., Salimans T., Sutskever I. Improving Language Understanding by Generative Pre-Training // OpenAI, 2018.

30. Liu Y., Ott M., Goyal N. et al. RoBERTa: A Robustly Optimized BERT Pretraining Approach // arXiv:1907.11692, 2019.

31. Raffel C., Shazeer N., Roberts A. et al. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer // JMLR. — 2020. — Vol. 21, № 140. — P. 1-67.

32. Touvron H., Lavril T., Izacard G. et al. LLaMA: Open and Efficient Foundation Language Models // arXiv:2302.13971, 2023.

33. Ouyang L., Wu J., Jiang X. et al. Training Language Models to Follow Instructions with Human Feedback // NeurIPS. — 2022. — Vol. 35. — P. 27730-27744.

34. Wei J., Wang X., Schuurmans D. et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models // NeurIPS. — 2022. — Vol. 35. — P. 24824-24837.

43. Thoppilan R., De Freitas D., Hall J. et al. LaMDA: Language Models for Dialog Applications // arXiv:2201.08239, 2022.

44. Anil R., Dai A.M., Firat O. et al. PaLM 2 Technical Report // arXiv:2305.10403, 2023.

45. Anthropic. Claude 3 Model Card. — Anthropic, 2024.

## Семантический поиск и FAISS

16. Johnson J., Douze M., Jégou H. Billion-scale similarity search with GPUs // IEEE Transactions on Big Data. — 2021. — Vol. 7, № 3. — P. 535-547.

17. Malkov Y.A., Yashunin D.A. Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs // IEEE TPAMI. — 2020. — Vol. 42, № 4. — P. 824-836.

35. Khattab O., Zaharia M. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT // SIGIR. — 2020. — P. 39-48.

36. Karpukhin V., Oguz B., Min S. et al. Dense Passage Retrieval for Open-Domain Question Answering // EMNLP. — 2020. — P. 6769-6781.

46. Jégou H., Douze M., Schmid C. Product Quantization for Nearest Neighbor Search // IEEE TPAMI. — 2011. — Vol. 33, № 1. — P. 117-128.

47. Babenko A., Lempitsky V. The Inverted Multi-Index // CVPR. — 2012. — P. 3069-3076.

48. Ge T., He K., Ke Q., Sun J. Optimized Product Quantization // IEEE TPAMI. — 2014. — Vol. 36, № 4. — P. 744-755.

49. Baranchuk D., Babenko A., Malkov Y. Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors // ECCV. — 2018. — P. 202-216.

50. Jayaram Subramanya S., Devvrit F., Simhadri H.V. et al. DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node // NeurIPS. — 2019. — Vol. 32. — P. 13766-13776.

51. Guo R., Sun P., Lindgren E. et al. Accelerating Large-Scale Inference with Anisotropic Vector Quantization // ICML. — 2020. — P. 3887-3896.

69. Douze M., Guzhva A., Deng C. et al. The Faiss Library // arXiv:2401.08281, 2024.

70. Wang L., Yang N., Huang X. et al. Text Embeddings by Weakly-Supervised Contrastive Pre-training // arXiv:2212.03533, 2022.

## Retrieval-Augmented Generation (RAG)

18. Lewis P., Perez E., Piktus A. et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks // NeurIPS. — 2020. — Vol. 33. — P. 9459-9474.

19. Gao L., Ma X., Lin J., Callan J. Precise Zero-Shot Dense Retrieval without Relevance Labels // ACL. — 2023. — P. 1762-1777.

37. Izacard G., Grave E. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering // EACL. — 2021. — P. 874-880.

38. Borgeaud S., Mensch A., Hoffmann J. et al. Improving Language Models by Retrieving from Trillions of Tokens // ICML. — 2022. — P. 2206-2240.

39. Guu K., Lee K., Tung Z. et al. REALM: Retrieval-Augmented Language Model Pre-Training // ICML. — 2020. — P. 3929-3938.

40. Shuster K., Poff S., Chen M. et al. Retrieval Augmentation Reduces Hallucination in Conversation // EMNLP Findings. — 2021. — P. 3784-3803.

41. Nakano R., Hilton J., Balaji S. et al. WebGPT: Browser-assisted Question-answering with Human Feedback // arXiv:2112.09332, 2021.

42. Menick J., Trebacz M., Mikulik V. et al. Teaching Language Models to Support Answers with Verified Quotes // arXiv:2203.11147, 2022.

64. Asai A., Wu Z., Wang Y. et al. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection // arXiv:2310.11511, 2023.

65. Peng B., Galley M., He P. et al. Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback // arXiv:2302.12813, 2023.

## Галлюцинации и надёжность LLM

63. Ji Z., Lee N., Frieske R. et al. Survey of Hallucination in Natural Language Generation // ACM Computing Surveys. — 2023. — Vol. 55, № 12. — P. 1-38.

68. Glaese A., McAleese N., Trębacz M. et al. Improving Alignment of Dialogue Agents via Targeted Human Judgements // arXiv:2209.14375, 2022.

## Метод Кано и продуктовая аналитика

20. Kano N. Attractive Quality and Must-be Quality // Journal of the Japanese Society for Quality Control. — 1984. — Vol. 14, № 2. — P. 39-48.

21. Matzler K., Hinterhuber H.H. How to Make Product Development Projects More Successful by Integrating Kano's Model // Technovation. — 1998. — Vol. 18, № 1. — P. 25-38.

61. Berger C., Blauth R., Boger D. et al. Kano's Methods for Understanding Customer-defined Quality // Center for Quality Management Journal. — 1993. — Vol. 2, № 4. — P. 3-36.

## Инфраструктура и технологии

22. MongoDB Inc. MongoDB Documentation. — https://docs.mongodb.com/, 2024.

23. Ramírez F. FastAPI: Modern, Fast Web Framework for Building APIs. — https://fastapi.tiangolo.com/, 2024.

24. Google Cloud. Cloud Run Documentation. — https://cloud.google.com/run/docs, 2024.

56. Martin R.C. Clean Architecture: A Craftsman's Guide to Software Structure and Design. — Prentice Hall, 2017. — 432 p.

## Рекомендательные системы

52. Chen Q., Zhao H., Li W. et al. Behavior Sequence Transformer for E-commerce Recommendation in Alibaba // DLP-KDD. — 2019. — P. 1-4.

53. Covington P., Adams J., Sargin E. Deep Neural Networks for YouTube Recommendations // RecSys. — 2016. — P. 191-198.

54. Gillick D., Presta A., Tomar G.S. End-to-End Retrieval in Continuous Space // arXiv:1811.08008, 2018.

## Информационный поиск и метрики

55. Manning C.D., Raghavan P., Schütze H. Introduction to Information Retrieval. — Cambridge University Press, 2008. — 544 p.

57. Bengio Y., Ducharme R., Vincent P., Jauvin C. A Neural Probabilistic Language Model // JMLR. — 2003. — Vol. 3. — P. 1137-1155.

62. Järvelin K., Kekäläinen J. Cumulated Gain-based Evaluation of IR Techniques // ACM TOIS. — 2002. — Vol. 20, № 4. — P. 422-446.

## Российские источники

66. Яков и Партнёры, Яндекс. Искусственный интеллект в России — 2025: Исследование рынка. — М., 2024.

67. Министерство труда и социальной защиты РФ. Прогноз потребности экономики в квалифицированных кадрах на период до 2030 года. — М., 2023.

---

**Всего источников:** 70

**Распределение по годам:**
- 2020-2025: 45 источников (64%)
- 2015-2019: 15 источников (21%)
- до 2015: 10 источников (14%)

**Распределение по типам:**
- Научные статьи (конференции, журналы): 50
- Технические отчёты и препринты: 12
- Документация и веб-ресурсы: 5
- Книги: 3
