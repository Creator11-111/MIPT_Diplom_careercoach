"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π —Å —Å–∞–π—Ç–∞ HH.ru

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç —Å–æ–±–∏—Ä–∞–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∏ –±–∞–Ω–∫–æ–≤—Å–∫–æ–≥–æ —Å–µ–∫—Ç–æ—Ä–∞
–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ —Ñ–æ—Ä–º–∞—Ç Parquet –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.

–í–ê–ñ–ù–û: HH.ru –∏–º–µ–µ—Ç API –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏. 
–ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π API, –∫–æ—Ç–æ—Ä—ã–π —Ç—Ä–µ–±—É–µ—Ç —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.
"""

from __future__ import annotations

import os
import time
import json
from typing import List, Dict, Any, Optional
from datetime import datetime
import requests
import polars as pl
from tqdm import tqdm


# –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–∞ HH.ru
FINANCIAL_CATEGORIES = [
    "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫",
    "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä",
    "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä",
    "–ë–∞–Ω–∫–æ–≤—Å–∫–∏–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç",
    "–ö—Ä–µ–¥–∏—Ç–Ω—ã–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç",
    "–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä",
    "–ê—É–¥–∏—Ç–æ—Ä",
    "–ë—É—Ö–≥–∞–ª—Ç–µ—Ä",
    "–†–∏—Å–∫-–º–µ–Ω–µ–¥–∂–µ—Ä",
    "–ö–∞–∑–Ω–∞—á–µ–π",
    "–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ –ú–°–§–û",
    "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∫–æ–Ω—Ç—Ä–æ–ª–µ—Ä",
    "–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–º—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é",
    "–ê–Ω–∞–ª–∏—Ç–∏–∫ –ø–æ –∫—Ä–µ–¥–∏—Ç–Ω—ã–º —Ä–∏—Å–∫–∞–º",
]


class HHVacancyScraper:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–∞–∫–∞–Ω—Å–∏–π —Å HH.ru —á–µ—Ä–µ–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π API
    
    –ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:
    1. –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π API HH.ru (–Ω–µ –ø–∞—Ä—Å–∏–Ω–≥ HTML)
    2. –ò—â–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≤–∏–¥–µ
    """
    
    def __init__(self, api_token: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä–∞–ø–µ—Ä–∞
        
        Args:
            api_token: API —Ç–æ–∫–µ–Ω –æ—Ç HH.ru (–µ—Å–ª–∏ –µ—Å—Ç—å)
                      –ï—Å–ª–∏ –Ω–µ—Ç - –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–µ–∑ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø—É–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        self.api_token = api_token
        self.base_url = "https://api.hh.ru"
        self.headers = {
            "User-Agent": "FinancialCareerCoach/1.0 (financial-coach-bot@example.com)"
        }
        if api_token:
            self.headers["Authorization"] = f"Bearer {api_token}"
        
        # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å API)
        self.delay_seconds = 0.2
        
    def search_vacancies(
        self, 
        text: str, 
        area: int = 1,  # 1 = –ú–æ—Å–∫–≤–∞, 2 = –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥, 113 = –†–æ—Å—Å–∏—è
        per_page: int = 100,
        max_pages: int = 20
    ) -> List[Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É –∑–∞–ø—Ä–æ—Å–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫")
            area: ID —Ä–µ–≥–∏–æ–Ω–∞ (1=–ú–æ—Å–∫–≤–∞, 2=–°–ü–±, 113=–†–æ—Å—Å–∏—è)
            per_page: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ (–º–∞–∫—Å 100)
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –æ –≤–∞–∫–∞–Ω—Å–∏—è—Ö
        """
        vacancies = []
        page = 0
        
        print(f"üîç –ò—â–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{text}'...")
        
        while page < max_pages:
            try:
                # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –∫ API
                params = {
                    "text": text,
                    "area": area,
                    "per_page": per_page,
                    "page": page,
                    "only_with_salary": False,  # –í–∫–ª—é—á–∞–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏ —Å –∑–∞—Ä–ø–ª–∞—Ç–æ–π –∏ –±–µ–∑
                }
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ API
                response = requests.get(
                    f"{self.base_url}/vacancies",
                    params=params,
                    headers=self.headers,
                    timeout=10
                )
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞
                if response.status_code == 429:
                    print(f"‚ö†Ô∏è  Rate limit –¥–æ—Å—Ç–∏–≥–Ω—É—Ç. –û–∂–∏–¥–∞–Ω–∏–µ 10 —Å–µ–∫—É–Ω–¥...")
                    time.sleep(10)
                    continue
                elif response.status_code != 200:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {response.status_code}")
                    if response.status_code >= 500:
                        print(f"‚è≥ –°–µ—Ä–≤–µ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞. –û–∂–∏–¥–∞–Ω–∏–µ 5 —Å–µ–∫—É–Ω–¥...")
                        time.sleep(5)
                        continue
                    break
                
                data = response.json()
                
                # –ï—Å–ª–∏ –≤–∞–∫–∞–Ω—Å–∏–π –Ω–µ—Ç - –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º
                if not data.get("items"):
                    break
                
                # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–∂–¥–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏
                for item in tqdm(data["items"], desc=f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page + 1}"):
                    vacancy_detail = self.get_vacancy_details(item["id"])
                    if vacancy_detail:
                        vacancies.append(vacancy_detail)
                    
                    # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    time.sleep(self.delay_seconds)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –µ—â–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                pages = data.get("pages", 0)
                found = data.get("found", 0)
                
                print(f"üìä –ù–∞–π–¥–µ–Ω–æ: {found} –≤–∞–∫–∞–Ω—Å–∏–π, —Å—Ç—Ä–∞–Ω–∏—Ü: {pages}, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(vacancies)}")
                
                if page >= pages - 1:
                    break
                
                page += 1
                time.sleep(self.delay_seconds)
                
            except requests.exceptions.RequestException as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {e}")
                print(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ 5 —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º...")
                time.sleep(5)
                continue
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {e}")
                print(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ 3 —Å–µ–∫—É–Ω–¥—ã –ø–µ—Ä–µ–¥ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ–º...")
                time.sleep(3)
                break
        
        return vacancies
    
    def get_vacancy_details(self, vacancy_id: str) -> Optional[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–∞–∫–∞–Ω—Å–∏–∏
        
        Args:
            vacancy_id: ID –≤–∞–∫–∞–Ω—Å–∏–∏ –Ω–∞ HH.ru
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –æ –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            response = requests.get(
                f"{self.base_url}/vacancies/{vacancy_id}",
                headers=self.headers,
                timeout=10
            )
            
            if response.status_code == 429:
                print(f"‚ö†Ô∏è  Rate limit –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy_id}. –û–∂–∏–¥–∞–Ω–∏–µ...")
                time.sleep(5)
                # –ü–æ–≤—Ç–æ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å –æ–¥–∏–Ω —Ä–∞–∑
                response = requests.get(
                    f"{self.base_url}/vacancies/{vacancy_id}",
                    headers=self.headers,
                    timeout=10
                )
            
            if response.status_code != 200:
                return None
            
            data = response.json()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω—É–∂–Ω—ã–µ –ø–æ–ª—è
            salary = data.get("salary")
            salary_str = ""
            if salary:
                if salary.get("from") and salary.get("to"):
                    salary_str = f"{salary['from']} - {salary['to']} {salary.get('currency', 'RUR')}"
                elif salary.get("from"):
                    salary_str = f"–æ—Ç {salary['from']} {salary.get('currency', 'RUR')}"
                elif salary.get("to"):
                    salary_str = f"–¥–æ {salary['to']} {salary.get('currency', 'RUR')}"
            
            # –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã
            experience = data.get("experience", {})
            experience_str = experience.get("name", "") if experience else ""
            
            # –ù–∞–≤—ã–∫–∏
            skills = [skill.get("name", "") for skill in data.get("key_skills", [])]
            skills_str = ", ".join(skills) if skills else ""
            
            # –û–ø–∏—Å–∞–Ω–∏–µ (—É–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏)
            description = data.get("description", "")
            # –ü—Ä–æ—Å—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞ HTML (–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å beautifulsoup4 –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π)
            description = description.replace("<p>", "").replace("</p>", "\n")
            description = description.replace("<br>", "\n").replace("<br/>", "\n")
            description = description.replace("<li>", "- ").replace("</li>", "\n")
            description = description.replace("<ul>", "").replace("</ul>", "")
            description = description.replace("<strong>", "").replace("</strong>", "")
            description = description.replace("<em>", "").replace("</em>", "")
            # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
            description = " ".join(description.split())
            
            # –ö–æ–º–ø–∞–Ω–∏—è
            employer = data.get("employer", {})
            company = employer.get("name", "") if employer else ""
            
            # –õ–æ–∫–∞—Ü–∏—è
            area_data = data.get("area", {})
            location = area_data.get("name", "") if area_data else ""
            
            # –¢–∏–ø —Ä–∞–±–æ—Ç—ã
            employment = data.get("employment", {})
            job_type = employment.get("name", "") if employment else ""
            
            # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            published_at = data.get("published_at", "")
            
            return {
                "idx": int(vacancy_id),  # –ò—Å–ø–æ–ª—å–∑—É–µ–º ID –≤–∞–∫–∞–Ω—Å–∏–∏ –∫–∞–∫ –∏–Ω–¥–µ–∫—Å
                "‚Ññ": int(vacancy_id),
                "id": int(vacancy_id),
                "title": data.get("name", ""),
                "salary": salary_str,
                "experience": experience_str,
                "job_type": job_type,
                "description": description[:5000] if len(description) > 5000 else description,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                "key_skills": skills_str,
                "company": company,
                "location": location,
                "date_of_post": published_at,
                "type": "financial",  # –ú–∞—Ä–∫–µ—Ä –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π
                "hh_url": data.get("alternate_url", ""),
            }
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–µ—Ç–∞–ª–µ–π –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy_id}: {e}")
            return None
    
    def scrape_all_financial_vacancies(
        self, 
        area: int = 113,  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—â–µ–º –ø–æ –≤—Å–µ–π –†–æ—Å—Å–∏–∏
        max_per_category: int = 500
    ) -> List[Dict[str, Any]]:
        """
        –°–±–æ—Ä –≤—Å–µ—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        
        Args:
            area: ID —Ä–µ–≥–∏–æ–Ω–∞ (113 = –†–æ—Å—Å–∏—è)
            max_per_category: –ú–∞–∫—Å–∏–º—É–º –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            
        Returns:
            –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π
        """
        all_vacancies = []
        seen_ids = set()  # –î–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        
        print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π...")
        print(f"üìã –ö–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –ø–æ–∏—Å–∫–∞: {len(FINANCIAL_CATEGORIES)}")
        
        for category in FINANCIAL_CATEGORIES:
            print(f"\n{'='*60}")
            print(f"üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")
            print(f"{'='*60}")
            
            vacancies = self.search_vacancies(
                text=category,
                area=area,
                per_page=100,
                max_pages=max_per_category // 100
            )
            
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            for vacancy in vacancies:
                vacancy_id = vacancy.get("id")
                if vacancy_id and vacancy_id not in seen_ids:
                    seen_ids.add(vacancy_id)
                    all_vacancies.append(vacancy)
            
            print(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category}': {len(vacancies)}")
            print(f"üìä –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(all_vacancies)}")
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
            time.sleep(1)
        
        print(f"\nüéâ –ò—Ç–æ–≥–æ —Å–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π: {len(all_vacancies)}")
        return all_vacancies
    
    def save_to_parquet(self, vacancies: List[Dict[str, Any]], output_path: str):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–π –≤ Parquet —Ñ–æ—Ä–º–∞—Ç
        
        Args:
            vacancies: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞
        """
        if not vacancies:
            print("‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return
        
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º {len(vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π –≤ {output_path}...")
        
        try:
            # –°–æ–∑–¥–∞–µ–º DataFrame –∏–∑ –¥–∞–Ω–Ω—ã—Ö
            df = pl.DataFrame(vacancies)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª, –ø–æ—Ç–æ–º –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º (–∞—Ç–æ–º–∞—Ä–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è)
            temp_path = output_path + ".tmp"
            df.write_parquet(temp_path, compression="snappy")
            
            # –ê—Ç–æ–º–∞—Ä–Ω–æ–µ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ
            if os.path.exists(output_path):
                os.remove(output_path)
            os.rename(temp_path, output_path)
            
            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_path}")
            print(f"üìä –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ
            temp_path = output_path + ".tmp"
            if os.path.exists(temp_path):
                os.remove(temp_path)
            raise


def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–∫—Ä–∏–ø—Ç–∞
    """
    print("="*60)
    print("üè¶ –ü–ê–†–°–ò–ù–ì –§–ò–ù–ê–ù–°–û–í–´–• –í–ê–ö–ê–ù–°–ò–ô –° HH.RU")
    print("="*60)
    print()
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    os.makedirs("financial_coach/data", exist_ok=True)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∫—Ä–∞–ø–µ—Ä
    # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –î–ª—è —Ä–∞–±–æ—Ç—ã –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å API —Ç–æ–∫–µ–Ω –Ω–∞ https://hh.ru/oauth/applications
    scraper = HHVacancyScraper(api_token=None)  # –ú–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å —Ç–æ–∫–µ–Ω, –µ—Å–ª–∏ –µ—Å—Ç—å
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤–∞–∫–∞–Ω—Å–∏–∏
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—â–µ–º –ø–æ –≤—Å–µ–π –†–æ—Å—Å–∏–∏ (area=113)
    # –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞ 1 (–ú–æ—Å–∫–≤–∞) –∏–ª–∏ 2 (–°–ü–±)
    vacancies = scraper.scrape_all_financial_vacancies(
        area=113,  # 113 = –†–æ—Å—Å–∏—è, 1 = –ú–æ—Å–∫–≤–∞, 2 = –°–ü–±
        max_per_category=500  # –ú–∞–∫—Å–∏–º—É–º 500 –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Parquet
    output_path = "financial_coach/data/financial_vacancies.parquet"
    scraper.save_to_parquet(vacancies, output_path)
    
    print("\n" + "="*60)
    print("‚úÖ –ì–û–¢–û–í–û! –í–∞–∫–∞–Ω—Å–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.")
    print("="*60)
    print(f"\nüìù –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    print(f"   python financial_coach/data_parsing/generate_embeddings.py")


if __name__ == "__main__":
    main()














