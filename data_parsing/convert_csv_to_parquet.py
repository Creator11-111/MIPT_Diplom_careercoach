"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ CSV –±–∞–∑—ã HeadHunter –≤ Parquet —Ñ–æ—Ä–º–∞—Ç

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç:
1. –ß–∏—Ç–∞–µ—Ç CSV —Ñ–∞–π–ª Raw_Jobs.csv —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏
2. –§–∏–ª—å—Ç—Ä—É–µ—Ç —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
3. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç Parquet —Å –Ω—É–∂–Ω—ã–º–∏ –ø–æ–ª—è–º–∏
4. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ financial_coach/data/financial_vacancies.parquet

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python financial_coach/data_parsing/convert_csv_to_parquet.py
"""

from __future__ import annotations

import os
from pathlib import Path
from typing import Any, Dict, List, Optional
import re

import pandas as pd
import polars as pl
from tqdm import tqdm

# –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
FINANCIAL_KEYWORDS = [
    "—Ñ–∏–Ω–∞–Ω—Å",
    "–±–∞–Ω–∫",
    "–±—É—Ö–≥–∞–ª—Ç–µ—Ä",
    "–∞—É–¥–∏—Ç–æ—Ä",
    "–∫–∞–∑–Ω–∞—á–µ–π",
    "–∫—Ä–µ–¥–∏—Ç",
    "–∏–Ω–≤–µ—Å—Ç–∏—Ü",
    "—Ä–∏—Å–∫-–º–µ–Ω–µ–¥–∂",
    "–±–∞–Ω–∫–æ–≤—Å–∫",
    "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫",
    "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä",
    "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä",
    "cfo",
    "–±–∞–Ω–∫–æ–≤—Å–∫–∏–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç",
    "–∫—Ä–µ–¥–∏—Ç–Ω—ã–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç",
    "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä",
    "–∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–æ –∫—Ä–µ–¥–∏—Ç–Ω—ã–º —Ä–∏—Å–∫–∞–º",
    "—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ –º—Å—Ñ–æ",
    "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∫–æ–Ω—Ç—Ä–æ–ª–µ—Ä",
    "—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–º—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é",
    "treasurer",
    "accountant",
    "auditor",
    "financial analyst",
    "financial manager",
    "bank",
    "credit",
    "investment",
    "—Ä–∏—Å–∫-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç",
    "—É–ø—Ä–∞–≤–ª–µ–Ω—á–µ—Å–∫–∏–π —É—á–µ—Ç",
    "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π —É—á–µ—Ç",
    "–±—é–¥–∂–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ",
    "—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ",
    "—Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å",
]


def normalize_column_name(col_name: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏."""
    normalized = col_name.strip().lower().replace(" ", "_")
    normalized = re.sub(r"[^\w_]", "", normalized)
    return normalized


def build_column_mapping(df_columns: List[str]) -> Dict[str, str]:
    """–ò—â–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –∫–æ–ª–æ–Ω–∫–∞–º–∏ CSV –∏ —Ç—Ä–µ–±—É–µ–º—ã–º–∏ –ø–æ–ª—è–º–∏."""
    normalized_cols = {normalize_column_name(col): col for col in df_columns}

    required = {
        "idx": ["idx", "id", "–Ω–æ–º–µ—Ä", "number", "‚Ññ"],
        "title": ["title", "–Ω–∞–∑–≤–∞–Ω–∏–µ", "name", "–¥–æ–ª–∂–Ω–æ—Å—Ç—å", "position", "job_title"],
        "description": ["description", "–æ–ø–∏—Å–∞–Ω–∏–µ", "–æ–ø–∏—Å–∞–Ω–∏–µ_–≤–∞–∫–∞–Ω—Å–∏–∏", "—Ç–µ–∫—Å—Ç", "text"],
        "company": ["company", "–∫–æ–º–ø–∞–Ω–∏—è", "employer", "—Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—å"],
        "location": ["location", "–ª–æ–∫–∞—Ü–∏—è", "–≥–æ—Ä–æ–¥", "city", "area", "—Ä–µ–≥–∏–æ–Ω"],
        "salary": ["salary", "–∑–∞—Ä–ø–ª–∞—Ç–∞", "–æ–∫–ª–∞–¥", "payment"],
        "experience": ["experience", "–æ–ø—ã—Ç", "–æ–ø—ã—Ç_—Ä–∞–±–æ—Ç—ã", "experience_required"],
        "key_skills": ["key_skills", "–Ω–∞–≤—ã–∫–∏", "skills", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è", "requirements", "–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏"],
        "job_type": ["job_type", "—Ç–∏–ø_—Ä–∞–±–æ—Ç—ã", "employment", "–∑–∞–Ω—è—Ç–æ—Å—Ç—å", "—Ñ–æ—Ä–º–∞—Ç"],
        "date_of_post": ["date_of_post", "–¥–∞—Ç–∞", "date", "published_at", "–¥–∞—Ç–∞_–ø—É–±–ª–∏–∫–∞—Ü–∏–∏"],
    }

    mapping: Dict[str, str] = {}
    for target, candidates in required.items():
        for candidate in candidates:
            key = normalize_column_name(candidate)
            if key in normalized_cols:
                mapping[target] = normalized_cols[key]
                break
    return mapping


def is_financial_vacancy(row: Dict[str, Any], title_col: str, desc_col: Optional[str]) -> bool:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ –≤–∞–∫–∞–Ω—Å–∏—è –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–º—É —Å–µ–∫—Ç–æ—Ä—É."""
    text_parts = [str(row.get(title_col, "") or "").lower()]
    if desc_col:
        text_parts.append(str(row.get(desc_col, "") or "").lower())
    text = " ".join(text_parts)

    return any(keyword in text for keyword in FINANCIAL_KEYWORDS)


def convert_csv_to_parquet(csv_path: str, output_path: str, sample_size: Optional[int] = None) -> None:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç CSV –≤ Parquet, –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–≤ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏."""
    print("=" * 60)
    print("üîÑ –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø CSV –í PARQUET")
    print("=" * 60)
    print()

    if not os.path.exists(csv_path):
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {csv_path}")
        return

    print(f"üìÇ –ß–∏—Ç–∞—é CSV: {csv_path}")

    chunk_size = 10_000
    total_rows = 0
    financial_rows = 0
    vacancies: List[Dict[str, Any]] = []

    try:
        preview = pd.read_csv(
            csv_path,
            nrows=1_000,
            encoding="utf-8",
            low_memory=False,
            on_bad_lines="skip",
            sep=";",
        )
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏: {list(preview.columns)}")

        column_mapping = build_column_mapping(list(preview.columns))
        print("üìã –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–æ–Ω–æ–∫:")
        for target, source in column_mapping.items():
            print(f"   {target} ‚Üê {source}")
        print()

        title_col = column_mapping.get("title") or preview.columns[0]
        desc_col = column_mapping.get("description")

        iterator = pd.read_csv(
            csv_path,
            chunksize=chunk_size,
            encoding="utf-8",
            low_memory=False,
            on_bad_lines="skip",
            nrows=sample_size,
            sep=";",
        )

        for chunk_index, chunk in enumerate(iterator, start=1):
            print(f"üì¶ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –±–ª–æ–∫ {chunk_index} ({len(chunk)} —Å—Ç—Ä–æ–∫)")
            for _, row in tqdm(chunk.iterrows(), total=len(chunk), desc=f"–ë–ª–æ–∫ {chunk_index}"):
                total_rows += 1
                row_dict = row.to_dict()

                if is_financial_vacancy(row_dict, title_col, desc_col):
                    financial_rows += 1
                    idx_value = row_dict.get(column_mapping.get("idx", "id"), total_rows)
                    vacancy = {
                        "idx": int(idx_value),
                        "‚Ññ": int(idx_value),
                        "id": int(idx_value),
                        "title": str(row_dict.get(title_col, "") or ""),
                        "salary": str(row_dict.get(column_mapping.get("salary", ""), "") or ""),
                        "experience": str(row_dict.get(column_mapping.get("experience", ""), "") or ""),
                        "job_type": str(row_dict.get(column_mapping.get("job_type", ""), "") or ""),
                        "description": str(row_dict.get(desc_col, "") or "") if desc_col else "",
                        "key_skills": str(row_dict.get(column_mapping.get("key_skills", ""), "") or ""),
                        "company": str(row_dict.get(column_mapping.get("company", ""), "") or ""),
                        "location": str(row_dict.get(column_mapping.get("location", ""), "") or ""),
                        "date_of_post": str(row_dict.get(column_mapping.get("date_of_post", ""), "") or ""),
                        "type": "financial",
                    }
                    vacancies.append(vacancy)

            print(f"   ‚úÖ –§–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞–π–¥–µ–Ω–æ: {financial_rows} –∏–∑ {total_rows}")

        print("=" * 60)
        print(f"üìä –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {total_rows}")
        print(f"üìä –§–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π: {financial_rows}")
        print("=" * 60)

        if not vacancies:
            print("‚ùå –§–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É CSV.")
            return

        output_dir = os.path.dirname(output_path)
        os.makedirs(output_dir, exist_ok=True)

        df_final = pl.DataFrame(vacancies)
        df_final.write_parquet(output_path, compression="snappy")

        size_mb = os.path.getsize(output_path) / 1024 / 1024
        print(f"‚úÖ Parquet —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path} ({size_mb:.2f} MB)")
        print()
        print("üìù –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: python financial_coach/data_parsing/generate_embeddings.py")

    except Exception as exc:  # noqa: BLE001
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {exc}")
        raise


def main() -> None:
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞."""
    project_root = Path(__file__).parent.parent.parent
    candidates = [
        project_root / "Raw_Jobs.csv",
        Path("Raw_Jobs.csv"),
    ]

    csv_path = next((path for path in candidates if path.exists()), None)
    if csv_path is None:
        print("‚ùå –§–∞–π–ª Raw_Jobs.csv –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–º–µ—Å—Ç–∏—Ç–µ –µ–≥–æ –≤ –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞.")
        return

    output_path = Path(__file__).parent.parent / "data" / "financial_vacancies.parquet"
    convert_csv_to_parquet(str(csv_path), str(output_path))


if __name__ == "__main__":
    main()


